{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e92e839a-d375-4991-9632-13955346affc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import xlsxwriter\n",
    "import os\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook'\n",
    "import plotly.express as px\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# @changes from inna\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91584b18-4524-49a0-adbc-27cd5120e34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(folderPathTrain, folderPathTest):\n",
    "    print('load new files')\n",
    "    df_categorical = pd.read_excel(f'{folderPathTrain}/TRAIN_CATEGORICAL_METADATA_new.xlsx')\n",
    "    df_matrices = pd.read_csv(f'{folderPathTrain}/TRAIN_FUNCTIONAL_CONNECTOME_MATRICES_new_36P_Pearson.csv')\n",
    "    df_quant = pd.read_excel(f'{folderPathTrain}/TRAIN_QUANTITATIVE_METADATA_new.xlsx')\n",
    "    df_solutions = pd.read_excel(f'{folderPathTrain}/TRAINING_SOLUTIONS.xlsx')  \n",
    "    print('load test files')\n",
    "    df_categorical_test = pd.read_excel(f'{folderPathTest}/TEST_CATEGORICAL.xlsx')\n",
    "    df_matrices_test = pd.read_csv(f'{folderPathTest}/TEST_FUNCTIONAL_CONNECTOME_MATRICES.csv')\n",
    "    df_quant_test = pd.read_excel(f'{folderPathTest}/TEST_QUANTITATIVE_METADATA.xlsx')\n",
    "    return df_categorical,df_matrices,df_quant,df_solutions,df_categorical_test,df_matrices_test,df_quant_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e040883f-7790-4bfe-9e3e-97075fed55d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_data(categorical,matrices,quantitative,solutions=None):\n",
    "    print('joining data frames')\n",
    "    cat_quant = pd.merge(categorical, quantitative , on ='participant_id', how ='inner')\n",
    "    cat_quant_mat = pd.merge(cat_quant, matrices , on ='participant_id', how ='inner')\n",
    "    if isinstance(solutions, pd.DataFrame):\n",
    "        cat_quant_mat_sols = pd.merge(cat_quant_mat, solutions , on ='participant_id', how ='inner')\n",
    "        return cat_quant_mat_sols\n",
    "    else:    \n",
    "        return cat_quant_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e091d34b-b925-4622-9995-73a2eff9ce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_classifer(objective = 'binary:logistic', max_depth=5,learning_rate=0.1,n_estimators=100):\n",
    "    print('xgboost_classifer')\n",
    "    # Initialize the base classifier\n",
    "    classifier = XGBClassifier(objective=objective, \\\n",
    "                               n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth)\n",
    "    multioutput_classifier = MultiOutputClassifier(classifier)\n",
    "    return multioutput_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "703bef88-dd2d-4dfd-b0dc-900b2f7f0d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,X_test):\n",
    "    print('predict with the model')\n",
    "    X_test_data  = X_test.drop(columns = ['participant_id'] )\n",
    "    y_pred = model.predict(X_test_data)\n",
    "    predictions_df = pd.DataFrame(\n",
    "        y_pred,\n",
    "        columns=['Predicted_Gender', 'Predicted_ADHD']\n",
    "    )\n",
    "    return predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c685473c-278b-46cd-96d6-d551b59ed902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(y_test,y_pred):\n",
    "    print('calculate score with prediction vs true values')\n",
    "    y_test_results  = y_test.drop(columns = ['participant_id'] )\n",
    "    accuracy = accuracy_score(y_test_results, y_pred)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6fde7e0-0a6f-4792-acfe-5da22cedf397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_data(X,Y):\n",
    "    print('split the train and test data')\n",
    "    X_train_data, X_test_data, y_train_data, y_test_data = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "    return X_train_data, X_test_data, y_train_data, y_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2f1be46-f7fc-4b14-9f48-e3bd237e6c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_output_accuracy(y_true, y_pred):\n",
    "    print('multi_output_accuracy')\n",
    "    # Ensure y_true and y_pred are NumPy arrays\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    # Compute accuracy for each target variable and return the mean\n",
    "    return np.mean([accuracy_score(y_true[:, i], y_pred[:, i]) for i in range(y_true.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5118ade-a2dc-4205-bceb-211daaf6ee60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_cross_validation(X,Y,model):\n",
    "    # Perform cross-validation on the training data\n",
    "    X_train_cv  = X.drop(columns = ['participant_id'] )\n",
    "    y_train_cv  = Y.drop(columns = ['participant_id'] )\n",
    "    # Create a scorer using scikit-learn's make_scorer\n",
    "    multi_output_scorer = make_scorer(multi_output_accuracy)\n",
    "    cv_scores = cross_val_score(model, X_train_cv, y_train_cv, cv=5, scoring=multi_output_scorer)\n",
    "    \n",
    "    # Output the cross-validation results\n",
    "    print(\"Cross-validation scores for each fold:\", cv_scores)\n",
    "    print(\"Mean CV score:\", f'Mean Accuracy: {np.mean(cv_scores) * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2ebed42-c23c-443b-ad4b-c18c54c511fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_matrices_data(df_matrices_new):\n",
    "    print('starting pca analysis')\n",
    "    df_matrices_for_pca = df_matrices_new.drop(columns = ['participant_id'] )\n",
    "    # PCA df with index preserved as index\n",
    "    \n",
    "    original_index = df_matrices_for_pca.index\n",
    "    \n",
    "    # 1. Standardize the data (excluding the first column)\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df_matrices_for_pca)\n",
    "    \n",
    "    # 2. Apply PCA\n",
    "    # Start with a smaller number of components for exploration\n",
    "    pca = PCA(n_components=1000)  # Adjust based on your needs\n",
    "    pca_result = pca.fit_transform(scaled_data)\n",
    "    \n",
    "    # 3. Analyze explained variance\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    cumulative_variance = np.cumsum(explained_variance)\n",
    "    \n",
    "    # 5. Find number of components for desired variance (e.g., 80%)\n",
    "    n_components_80 = np.argmax(cumulative_variance >= 0.8) + 1\n",
    "    print(f\"Number of components needed for 80% variance: {n_components_80}\")\n",
    "    \n",
    "    # 6. Re-run PCA with the optimal number of components\n",
    "    pca_final = PCA(n_components=n_components_80)\n",
    "    pca_result_final = pca_final.fit_transform(scaled_data)\n",
    "    \n",
    "    # 7. Create a DataFrame with the PCA results\n",
    "    pca_df = pd.DataFrame(\n",
    "        data=pca_result_final,\n",
    "        columns=[f'PC{i+1}' for i in range(n_components_80)],\n",
    "        index=original_index\n",
    "    )\n",
    "\n",
    "    # 8 \n",
    "    pca_df['participant_id'] = df_matrices_new['participant_id']\n",
    "    \n",
    "    return pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d493e54b-c66d-4853-be1a-fe54e93d79bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_quant_data(df_quant_new):\n",
    "    print('starting quant data scaling')\n",
    "    df_quant_scaled_dropped = df_quant_new.drop(columns = ['participant_id'] )\n",
    "    df_quant_scaled_dropped = pd.DataFrame(df_quant_scaled_dropped)\n",
    "    scaler = StandardScaler()\n",
    "    df_quant_scaled = scaler.fit_transform(df_quant_scaled_dropped)\n",
    "    df_quant_scaled = pd.DataFrame(df_quant_scaled)\n",
    "    df_quant_scaled['participant_id'] = df_quant_new['participant_id']\n",
    "\n",
    "    #select specific columns only for classifier\n",
    "    df_quant_scaled_selected = df_quant_scaled.iloc[:,4:]\n",
    "    return df_quant_scaled_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02fb1674-717c-4fc9-acb5-01dd4eb46f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_categorical_data(df_categorical_new):\n",
    "    print('starting categorical data encoding')\n",
    "    # One-Hot Encoding (nominal)\n",
    "    onehot_encoder = OneHotEncoder(sparse_output=False, drop='first') #drop first to prevent multicollinearity\n",
    "    nominal_cols = ['MRI_Track_Scan_Location', 'Basic_Demos_Study_Site', 'PreInt_Demos_Fam_Child_Race', 'PreInt_Demos_Fam_Child_Ethnicity']\n",
    "    onehot_encoded = onehot_encoder.fit_transform(df_categorical_new[nominal_cols])\n",
    "    onehot_df = pd.DataFrame(onehot_encoded, columns=onehot_encoder.get_feature_names_out(nominal_cols))\n",
    "    encoded_df = pd.concat([df_categorical_new, onehot_df], axis=1)\n",
    "    encoded_df = encoded_df.drop(nominal_cols, axis=1)\n",
    "\n",
    "    # Handle NaN and 0.0 values (imputation example)\n",
    "    encoded_df['Barratt_Barratt_P1_Edu'] = encoded_df['Barratt_Barratt_P1_Edu'].fillna(encoded_df['Barratt_Barratt_P1_Edu'].median())\n",
    "    encoded_df['Barratt_Barratt_P2_Edu'] = encoded_df['Barratt_Barratt_P2_Edu'].fillna(encoded_df['Barratt_Barratt_P2_Edu'].median())\n",
    "    \n",
    "    encoded_df['Barratt_Barratt_P1_Edu'] = encoded_df['Barratt_Barratt_P1_Edu'].replace(0.0, encoded_df['Barratt_Barratt_P1_Edu'].median())\n",
    "    encoded_df['Barratt_Barratt_P2_Edu'] = encoded_df['Barratt_Barratt_P2_Edu'].replace(0.0, encoded_df['Barratt_Barratt_P2_Edu'].median())\n",
    "\n",
    "    # Ordinal Encoding (ordinal)\n",
    "    ordinal_encoder = OrdinalEncoder(categories=[[ 3, 6, 9, 12, 15, 18, 21],[ 3, 6, 9, 12, 15, 18, 21] ])\n",
    "    ordinal_cols = ['Barratt_Barratt_P1_Edu', 'Barratt_Barratt_P2_Edu']\n",
    "    encoded_df[ordinal_cols] = ordinal_encoder.fit_transform(encoded_df[ordinal_cols])\n",
    "\n",
    "    # Handle NaN and 0.0 values (imputation example)\n",
    "    encoded_df['Barratt_Barratt_P1_Occ'] = encoded_df['Barratt_Barratt_P1_Occ'].fillna(encoded_df['Barratt_Barratt_P1_Occ'].median())\n",
    "    encoded_df['Barratt_Barratt_P2_Occ'] = encoded_df['Barratt_Barratt_P2_Occ'].fillna(encoded_df['Barratt_Barratt_P2_Occ'].median())\n",
    "    encoded_df['Barratt_Barratt_P1_Occ'] = encoded_df['Barratt_Barratt_P1_Occ'].replace(0.0, encoded_df['Barratt_Barratt_P1_Occ'].median())\n",
    "    encoded_df['Barratt_Barratt_P2_Occ'] = encoded_df['Barratt_Barratt_P2_Occ'].replace(0.0, encoded_df['Barratt_Barratt_P2_Occ'].median())\n",
    "    \n",
    "    # Ordinal Encoding (ordinal)\n",
    "    ordinal_encoder = OrdinalEncoder(categories=[[0, 5, 10, 15, 20, 25, 30, 35, 40, 45], [0, 5, 10, 15, 20, 25, 30, 35, 40, 45]])\n",
    "    ordinal_cols = ['Barratt_Barratt_P1_Occ', 'Barratt_Barratt_P2_Occ']\n",
    "    encoded_df[ordinal_cols] = ordinal_encoder.fit_transform(encoded_df[ordinal_cols])\n",
    "\n",
    "    encoded_df_modified = encoded_df.drop(columns = ['Basic_Demos_Enroll_Year'])\n",
    "    return encoded_df_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7277fc20-0e72-40d7-8b8e-c3cbec6adf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_train(X = None, Y =None,max_depth= None,learning_rate = None,n_estimators = None):\n",
    "    print('starting training')\n",
    "    print('setting tuning params')\n",
    "    classifier = xgboost_classifer(max_depth=max_depth,learning_rate=learning_rate,n_estimators=n_estimators)\n",
    "    print('splitting to test and train')\n",
    "    X_train_data, X_test_data, y_train_data, y_test_data = split_train_data(X, Y)\n",
    "    \n",
    "    print('training the model')\n",
    "    X_train  = X_train_data.drop(columns = ['participant_id'] )\n",
    "    y_train  = y_train_data.drop(columns = ['participant_id'] )\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    print('setting cross validation classifier with tuning params')\n",
    "    classifier_cv = xgboost_classifer(max_depth=max_depth,learning_rate=learning_rate,n_estimators=n_estimators)\n",
    "    print('start cross validation')\n",
    "    do_cross_validation(X,Y,classifier_cv)\n",
    "    \n",
    "    print('check accuracy')\n",
    "    y_pred = predict(classifier,X_test_data)\n",
    "    print('calculate score')\n",
    "    accuracy = calculate_score(y_test_data,y_pred)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb7b6290-0c8a-4e49-92dc-9f1e7b43e25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_test(classifier = None,X = None):\n",
    "    print('start testing')\n",
    "    Y = predict(classifier,X)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966c85c6-d929-40f6-b84e-444e006dff0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ebf7e6-cec8-4b2f-8e46-bb4735d36503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9eaf6668-5462-4f26-a954-441a810fc145",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2fc71f50-a4cf-4da3-b432-6bd5735c5d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load new files\n",
      "load test files\n"
     ]
    }
   ],
   "source": [
    "folderPathTrain, folderPathTest = 'Datafiles/TRAIN_NEW/' , 'Datafiles/TEST/'\n",
    "df_categorical_new,df_matrices_new,df_quant_new,df_solutions_new,df_categorical_test,df_matrices_test,df_quant_test = load_data(folderPathTrain, folderPathTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "692f0a49-d8ba-45ca-9399-be5c62be30e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting pca analysis\n",
      "Number of components needed for 80% variance: 464\n",
      "starting quant data scaling\n",
      "starting categorical data encoding\n"
     ]
    }
   ],
   "source": [
    "pca_df_train = transform_matrices_data(df_matrices_new)\n",
    "quant_df_train = transform_quant_data(df_quant_new)\n",
    "cat_df_train = transform_categorical_data(df_categorical_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7460ff98-62e3-46da-8457-5cc1faffd4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joining data frames\n"
     ]
    }
   ],
   "source": [
    "joined_training_data = join_data(cat_df_train,pca_df_train,quant_df_train,df_solutions_new)\n",
    "X = joined_training_data.drop(columns = ['ADHD_Outcome','Sex_F'] )\n",
    "Y = joined_training_data[['participant_id','ADHD_Outcome','Sex_F']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5e98fd11-ac76-4320-aa9d-cf38c73373c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1213, 504)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a4b1e837-581f-4a52-92f0-e91ceb493560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "participant_id    1213\n",
       "ADHD_Outcome      1213\n",
       "Sex_F             1213\n",
       "dtype: int64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d41c5437-55b9-4663-a914-93fab0edfc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "setting tuning params\n",
      "xgboost_classifer\n",
      "splitting to test and train\n",
      "split the train and test data\n",
      "training the model\n",
      "setting cross validation classifier with tuning params\n",
      "xgboost_classifer\n",
      "start cross validation\n",
      "multi_output_accuracy\n",
      "multi_output_accuracy\n",
      "multi_output_accuracy\n",
      "multi_output_accuracy\n",
      "multi_output_accuracy\n",
      "Cross-validation scores for each fold: [0.72222222 0.75925926 0.69958848 0.75826446 0.71694215]\n",
      "Mean CV score: Mean Accuracy: 73.13%\n",
      "check accuracy\n",
      "predict with the model\n",
      "calculate score\n",
      "calculate score with prediction vs true values\n",
      "Accuracy: 58.44%\n"
     ]
    }
   ],
   "source": [
    "classifier_trained = xgboost_train(X = X, Y =Y,max_depth= 5,learning_rate = 0.1,n_estimators = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84a825c-de80-427c-b688-ea04164e758a",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a64d72b6-5ec5-430a-ab40-224140504761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting pca analysis\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "n_components=1000 must be between 0 and min(n_samples, n_features)=304 with svd_solver='full'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pca_df_test \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_matrices_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_matrices_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m quant_df_test \u001b[38;5;241m=\u001b[39m transform_quant_data(df_quant_test)\n\u001b[1;32m      3\u001b[0m cat_df_test \u001b[38;5;241m=\u001b[39m transform_categorical_data(df_categorical_test)\n",
      "Cell \u001b[0;32mIn[36], line 15\u001b[0m, in \u001b[0;36mtransform_matrices_data\u001b[0;34m(df_matrices_new)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 2. Apply PCA\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Start with a smaller number of components for exploration\u001b[39;00m\n\u001b[1;32m     14\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)  \u001b[38;5;66;03m# Adjust based on your needs\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m pca_result \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaled_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 3. Analyze explained variance\u001b[39;00m\n\u001b[1;32m     18\u001b[0m explained_variance \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mexplained_variance_ratio_\n",
      "File \u001b[0;32m/opt/anaconda3/envs/datathon25/lib/python3.12/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/datathon25/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/datathon25/lib/python3.12/site-packages/sklearn/decomposition/_pca.py:468\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    447\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;124;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 468\u001b[0m     U, S, _, X, x_is_centered, xp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m U \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m         U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/datathon25/lib/python3.12/site-packages/sklearn/decomposition/_pca.py:542\u001b[0m, in \u001b[0;36mPCA._fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;66;03m# Call different fits for either full or truncated SVD\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcovariance_eigh\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_array_api_compliant\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandomized\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_truncated(X, n_components, xp)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/datathon25/lib/python3.12/site-packages/sklearn/decomposition/_pca.py:556\u001b[0m, in \u001b[0;36mPCA._fit_full\u001b[0;34m(self, X, n_components, xp, is_array_api_compliant)\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    553\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmle\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is only supported if n_samples >= n_features\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    554\u001b[0m         )\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n_components \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_samples, n_features):\n\u001b[0;32m--> 556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be between 0 and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin(n_samples, n_features)=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(n_samples,\u001b[38;5;250m \u001b[39mn_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvd_solver=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m     )\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_ \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mmean(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# When X is a scipy sparse matrix, self.mean_ is a numpy matrix, so we need\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# to transform it to a 1D array. Note that this is not the case when X\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;66;03m# is a scipy sparse array.\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# TODO: remove the following two lines when scikit-learn only depends\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;66;03m# on scipy versions that no longer support scipy.sparse matrices.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: n_components=1000 must be between 0 and min(n_samples, n_features)=304 with svd_solver='full'"
     ]
    }
   ],
   "source": [
    "pca_df_test = transform_matrices_data(df_matrices_test)\n",
    "quant_df_test = transform_quant_data(df_quant_test)\n",
    "cat_df_test = transform_categorical_data(df_categorical_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be0eb8f-c52a-46e0-a697-d6aac72251fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = join_data(cat_df_test,pca_df_test,quant_df_test)\n",
    "xgboost_test(classifier = classifier_trained,X = X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bb88d3-6031-4e56-b62a-83b8edf45465",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed8f95c-4a9a-4fb2-b47e-8529aef96933",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ca262a-0a71-44f4-af18-855d63c5e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.describe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
